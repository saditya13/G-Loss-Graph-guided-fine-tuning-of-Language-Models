Reviewer KWQ9

We thank the reviewer for their thoughtful and constructive feedback and for recognizing the strengths of our work, including the paradigm shift from local to global semantic alignment, the design of the dynamic mini-batch graph that ensures memory efficiency and inductive learning, the integration of LPA into a differentiable loss formulation, and the comprehensive experiments across five diverse benchmarks and three Transformer models. We also appreciate the recognition of the two G-Loss variants (G-Loss-SQRT and G-Loss-O), which together balance computational efficiency and performance flexibility across different resource settings.

Dynamic graph stability

In G-Loss, the Gaussian kernel constrains edge weights within the bounded range (0,1], ensuring smooth and continuous updates to the graph as embeddings evolve. Empirically, we observe low variance across random seeds (≤1%), confirming that dynamic graph construction is stable in practice. We find the reviewer’s suggestion insightful; introducing an additional smoothing parameter or a moving-average mechanism to further stabilize updates is an interesting direction that we plan to explore in future work.

Convergence and theoretical soundness

The closed-form solution in G-Loss follows the standard formulation in graph-based semi-supervised learning (Zhu et al., 2003; Zhou et al., 2004). When similarities are computed via the Gaussian kernel, the resulting similarity matrix *W* is positive semi-definite, guaranteeing that the normalized transition matrix *T = D⁻¹W* (Section 2.3, LPA) produces a row-stochastic or substochastic block *T<sub>uu</sub>* for unlabeled nodes. Because *T<sub>uu</sub>* is substochastic, its spectral radius ρ(*T<sub>uu</sub>*) < 1 (Zhu et al., 2002), implying that (*I − T<sub>uu</sub>*) is non-singular. Consequently, the closed-form expression *F<sub>u</sub> = (I − T<sub>uu</sub>)⁻¹T<sub>ul</sub>Y<sub>l</sub>* is well-defined and numerically stable within our mini-batch setup. Moreover, since all entries of *T* are positive due to the Gaussian kernel, no isolated nodes occur, preserving valid propagation. A detailed mathematical explanation has been added in Appendix Section E, referencing the Neumann series expansion and spectral radius condition for substochastic matrices.

Domain generalization

While our current experiments are focused on text classification tasks, the G-Loss framework is conceptually general and can be applied wherever encoder-based embeddings are available for constructing dynamic graphs. We agree that our previous statement about cross-domain applicability was broader than our experiments support. Accordingly, we have revised the text to align our claims with demonstrated results while initiating preliminary extensions to other modalities such as vision-language encoders (e.g., CLIP) for future work.

Hyperparameter tuning guide

We have added a systematic guide in Appendix Section G that includes a complete list of hyperparameters, their roles, and empirically determined stable ranges (γ: 0.5–0.7, λ: 0.7–0.9, σ within ±0.5× of the optimal). This guide simplifies tuning and improves reproducibility across datasets. We are also exploring adaptive tuning strategies for σ and γ as part of ongoing work.

Summary

We thank the reviewer again for the positive and detailed evaluation. The additional theoretical clarifications, tuning guidelines, and planned extensions further strengthen the contributions of G-Loss as a stable, mathematically sound, and flexible fine-tuning objective that balances global semantic alignment with practical computational efficiency across diverse scenarios.
