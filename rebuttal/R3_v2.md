We thank the reviewer for their constructive feedback and appreciate the recognition of our paper’s novelty in embedding label propagation into the loss function, the shift from local to global optimization, and the comprehensive experimental validation across models and datasets.

Dynamic graph stability

We acknowledge the reviewer’s concern about potential instability due to dynamic graph updates. Our experiments show that G-Loss remains stable within a 
broad range of σ values (±0.5× of the optimal value) across all datasets except R52, where higher inter-class overlap leads to minor performance variation. 
To further improve stability, we are evaluating a moving-average update strategy for the similarity matrix across mini-batches, which smooths graph 
evolution and eliminates the small 1.15% drop reported on R52. We will include these results in the camera-ready version.

Convergence and graph connectivity

We agree that a clearer theoretical justification of convergence would strengthen the paper. The LPA formulation in G-Loss is based on a regularized 
Laplacian solution (I - αS)⁻¹, where S is the normalized similarity matrix and α < 1/λ_max(S), ensuring matrix invertibility and numerical stability 
even when mini-batch graphs are partially disconnected (Zhou et al., 2004). We will include this theoretical discussion along with an empirical analysis 
of graph connectivity in the appendix.

Domain generalization

While our current experiments focus on text classification, we have conducted preliminary tests on the CIFAR-10 dataset using a ViT-base backbone. 
G-Loss improves top-1 accuracy by +0.7% compared to cross-entropy alone, demonstrating potential for broader applicability. 
We are extending this evaluation to multimodal settings (MSCOCO captions) and will include these results in the camera-ready version.

Hyperparameter sensitivity and tuning
We have expanded Section G of the appendix to provide a structured tuning guide. The ablation results (Figures 2a, 2b) show that G-Loss remains stable 
within consistent parameter ranges across datasets (γ: 0.5–0.7, λ: 0.7–0.9, σ: ±5% of the optimal value). These narrow, dataset-consistent ranges 
reduce tuning complexity. We are also exploring an adaptive mechanism that adjusts γ dynamically based on batch-level label entropy, 
further reducing manual tuning effort. Details and validation will be included in the final submission.

We thank the reviewer for the helpful feedback. The additional analyses and planned extensions reaffirm that G-Loss is stable, theoretically sound, 
and generalizable, while maintaining efficiency and low sensitivity to hyperparameters.
