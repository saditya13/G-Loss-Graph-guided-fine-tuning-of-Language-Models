1.We thank the reviewer for their helpful feedback and appreciate the acknowledgement of clear motivation, comprehensive evaluation on integrated and standalone modes, and thorough analysis and ablation of the proposed G-Loss function.

2.**Marginal performance gains**: While the reported gains in accuracy and F1-score may appear modest, they are consistent across five benchmark datasets and three language models of varying sizes. The motivation is to introduce a loss function that extends beyond local pairwise optimization (as in Triplet or SCL losses) to incorporate global graph-structured relationships. This global formulation enables faster convergence (less epochs to convergence), enhances representation quality (better Silhouette score), incurs negligible computational overhead (less per epoch train time) due to the mini-batch scale of graph construction.

3.**Average over multiple random seeds**: As suggested, we reran the experiments for the integrated approach and report the mean and variance across three different random seeds in Table 2. The results remain consistent with our earlier findings, with G-Loss performing on par or outperforming other loss functions across all datasets and language models. Owing to the limited rebuttal timeframe and computational constraints, we will include the results for the standalone approach with multiple seeds in the camera-ready version. 

4.**Unquantified Computational Overhead**: We have added the average time per epoch and total training time in Table 3, which show that G-Loss achieves faster convergence with a comparable per-epoch training time. We also show a breakdown of the per-epoch timing in Figure 5, indicating that G-Loss requires $9.2$ seconds for the forward pass (BERT forward pass: $8.95$ sec, graph construction: $0.18$ sec, LPA operations: $0.07$ sec), compared to Supervised Contrastive Loss ($10.41$ sec), Triplet Loss ($9.15$ sec), and Cosine Similarity Loss ($9.09$ sec). These results demonstrate that G-Loss introduces negligible computational overhead from graph construction and matrix inversion operations within LPA.


5.**High Complexity in Hyperparameter Tuning**: While G-Loss introduces a few additional parameters, we have provided guidelines (section xxx) on how to tune them to ensure stable performance. We have added a table describing the systematic list of hyperparameters and optial ranges in table xxx. Our ablation study shows that the parameter stability window - where the model performs consistently well - is narrow ($\gamma$ - [$0.5-0.7$], $\lambda$ - [$0.7-0.9$]) and largely consistent across datasets, reducing the effective hyperparameter search space and minimizing the need for extensive tuning in classification tasks. Furthermore, we will include a tornado plot to illustrate the sensitivity of these parameters with respect to model performance and convergence time in camera ready version.

