We thank the reviewer for acknowledging the aspect that the proposed approach is simple to follow and is consistent in terms of performance.

1.**Global semantic consistency**: *Can you clarify the meaning of "global semantic consistency" -- is it the same as "better class clustering"?* 
By "global-semantic consistency" we meant that GLoss enforces structural alignment not just for the immediate neighbours (local consistency), but for multi-hop relationships that capture global structure in the document graph. That is, by looking at the relationships between the labelled and unlabelled datapoints in the graph simultaenusly instead of in pairs/triplets, the model is able to learn representations that exhibit a class-level coherence across the minibatch. Yes, it is rightly pointed out that global semantic consistency will result in better class clustering and separation at minibatch/global level.

*Why do other contrastive learning approaches not enforce global consistency in expectation?*
Contrastive losses such as supervised contrastive learning (SCL) operate through a local, pairwise mechanism that pulls positive pairs together while pushing negative pairs apart. While this approach can indeed achieve strong local alignment between individual samples, it does not explicitly enforce a global objective that simultaneously optimizes class-level alignment at the holistic level. GLoss addresses this limitation and promotes consistent class-level clustering and separation, thereby achieving global alignment rather than merely local pairwise consistency.

*Can you also provide plots for the other remaining datasets?*

*Instead of relying on visual observations, can we resort to using some clustering score to validate the claim?*
To quantitatively evaluate the clustering quality of the learned representations, we report the Silhouette coefficient for G-Loss and other baseline losses across all datasets in Table 3. 

2.**Computational efficiency**:
As suggested, we have added the average time per epoch and the total training time in Table 3, and a timing breakdown of training time for GLoss and other baselines in Figure 5 of the appendix.

3.**Downstream tasks & Language Models**: We completely agree that evaluating the performance of G-Loss on vision-language models and large language models (with >1B parameters) would further strengthen the study. However, due to the computational constraints and limited resources available during this work, our experiments were restricted to text classification tasks using models under 1B parameters. We plan to extend the implementation of G-Loss to vision and multimodal settings in future work.

5.**Why Gaussian kernel**: The Gaussian kernel was chosen because it is a well-established choice in graph-based semi-supervised learning (Zhu et al., 2003 [1]; Zhou et al., 2003[2]). Within GLoss, it contributes to the stability and invertibility of the LPA formulation by generating a positive semidefinite similarity matrix $W$. Moreover, it aligns naturally with the geometry of the embedding space, ensuring that nearby points form stronger connections while distant points' influence decays smoothly, thereby maintaining a balanced and well-structured graph.

6.**Why SGD**: We thank the reviewer for catching this inconsistency. We would like to clarify that all experiments were conducted using the Adam optimizer, which is the standard choice for fine-tuning transformer-based models [BERT, RoBERTa]. The mention of SGD in the manuscript was an inadvertent typographical error and the discrepancy arose from a documentation oversight during manuscript preparation. We have verified our training scripts and logs to confirm this, and we have corrected the optimizer description in the revised version. 

7.**Multiple plots**:



[1] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions (ICML- 2003).

[2] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch√∂lkopf. 2003. Learning with local and global consistency (NIPS 2003)
