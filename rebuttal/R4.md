We thank the reviewer for acknowledging the aspect that the proposed approach is simple to follow and is consistent in terms of performance.

1.**Global semantic consistency**: *Can you clarify the meaning of "global semantic consistency" -- is it the same as "better class clustering"?* 
By "global-semantic consistency" we meant that by looking at the relationships between the labelled and unlabelled datapoints in the graph simultaenusly instead of in pairs/triplets, the model is able to learned representations that exhibit a class-level coherence across the minibatch. Yes, it is rightly pointed out that global semantic consistency will result in better class clustering and separation at minibatch/global level, not merely at pairwise/triplet relationships.

*Why do other contrastive learning approaches not enforce global consistency in expectation?*
Contrastive losses such as supervised contrastive learning (SCL) operate through a local, pairwise mechanism that pulls positive pairs together while pushing negative pairs apart. While this approach can indeed achieve strong local alignment between individual samples, it does not explicitly enforce a global objective that simultaneously optimizes class-level alignment at the holistic level. G-Loss addresses this limitation and enforces a joint global objective ensuring class-level alignment over the whole graph, promoting batch-level cluster coherence and separation.

*Can you also provide plots for the other remaining datasets?*

*Instead of relying on visual observations, can we resort to using some clustering score to validate the claim?*
To quantitatively evaluate the clustering quality of the learned representations, we report the Silhouette coefficient for G-Loss and other baseline losses across all datasets in Table 3. 

2.**Computational efficiency**:
As suggested, we have included the average time per epoch and the total training time in Table 3, along with a detailed timing breakdown of the overall training process provided in Figure 5 of the appendix.

3.**Downstream tasks & Language Models**: We completely agree with the reviewer that evaluating the performance of G-Loss on vision-language models and large language models (with >1B parameters) would further strengthen the study. However, due to the computational constraints and limited resources available during this work, our experiments were restricted to text classification tasks using models under 1B parameters. We plan to extend the implementation of G-Loss to vision and multimodal settings in future work.

5.**Why Gaussian kernel**: The Gaussian kernel was chosen because it is a well-established choice in graph-based semi-supervised learning (Zhu et al., 2003; Zhou et al., 2004). Within G-Loss, it contributes to the stability and invertibility of the LPA formulation by generating a positive semidefinite similarity matrix $W$. Moreover, it aligns naturally with the geometry of the embedding space, ensuring that nearby points form stronger connections while distant points' influence decays smoothly, thereby maintaining a balanced and well-structured graph.

6.**Why SGD**: We thank the reviewer for catching this inconsistency. We would like to clarify that all experiments were conducted using the Adam optimizer, which is the standard choice for fine-tuning transformer-based models [BERT, RoBERTa]. The mention of SGD in the manuscript was an inadvertent typographical error and the discrepancy arose from a documentation oversight during manuscript preparation. We have verified our training scripts and logs to confirm this, and we have corrected the optimizer dexcription in the revised version. 

7.**Multiple plots**: