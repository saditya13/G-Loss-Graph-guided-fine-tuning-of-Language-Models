1. We appreciate the acknowledgement of our work's clear motivation, comprehensive evaluation, and thorough analysis of the proposed G-Loss function.
2. **Marginal performance gains**: Although the reported improvements appear marginal, they are consistent across the five benchmarks and three language models of varying sizes. Beyond accuracy/F1, embedding quality improves significantly, as shown by the silhouette score [Table3] and t-SNE visualization[figure4 in appendix], confirming the benefit of enforcing global semantic information. 

We reran our results with 3 random seeds, and the updated results are as follows:



### DistilBERT-base Results Across Datasets

| Loss Function     | MR (Acc / F1)      | R8 (Acc / F1)       | R52 (Acc / F1)      | 20NG (Acc / F1)     | Ohsumed (Acc / F1)   |
|-------------------|--------------------|----------------------|----------------------|----------------------|----------------------|
| CE                | 84.85±0.23 / 84.85±0.23 | 97.48±0.11 / 93.02±0.48 | 95.78±0.04 / 83.64±0.69 | 83.15±0.61 / 82.69±0.56 | 69.50±0.34 / 61.56±0.29 |
| **GLoss-SQRT + CE** | **85.14±0.33 / 85.14±0.34** | **97.46±0.17 / 93.62±0.69** | **96.13±0.34 / 83.93±0.35** | **83.61±0.23 / 83.16±0.20** | **69.93±0.38 / 62.19±0.21** |
| GLoss-O + CE      | 85.10±0.45 / 85.10±0.45 | 97.71±0.12 / 93.61±0.22 | 96.07±0.04 / 84.44±0.43 | 83.64±0.28 / 83.23±0.27 | 70.16±0.21 / 62.41±0.34 |
| SCL + CE          | 84.16±0.46 / 84.16±0.46 | 97.65±0.25 / 93.79±0.43 | 96.17±0.10 / 83.82±1.62 | 83.49±0.20 / 83.18±0.12 | 70.12±0.35 / 60.56±0.30 |
