1. We thank the reviewer for their valuable feedback and for acknowledging the novelty, clear motivation, and thoroughness of our experimental setup.

2. **Limited performance gains**: While the quantitative gains are modest, G-Loss contributes a mechanism for incorporating global structure into fine-tuning of language models - something existing loss formulations do not achieve. Its consistent improvements across models, faster convergence, and better embedding coherence suggest that the benefit lies in representation quality rather than raw accuracy/F1 measure.

3. **Statistical significance tests**: To address this, we repeated the integrated approach experiments using three different random seeds, and report the mean and variance across multiple runs in Table 2. As observed, G-Loss consistently matches or outperforms other methods across all datasets. Due to time and computational constraints during the rebuttal period, we will report the results for multi-seed evaluation of standalone mode in the camera-ready version. Furthermore, we will also run the statistical test (e.g. p-value) and report the results.

4. **Efficiency analysis**: We reports the average time per epoch and total training time in Table 3, showing that G-Loss converges faster while maintaining a comparable per-epoch training cost. We also provide a timing breakdown in Figure 5, where G-Loss records a total of $9.2$ seconds for forward pass (BERT forward: $8.95$ sec, graph construction: $0.18$ sec, LPA operations: $0.07$ sec), compared forward passes of Supervised Contrastive Loss ($10.41 $ sec), Triplet Loss ($9.15$ sec), and Cosine Similarity Loss ($9.09$ sec). These findings confirm that the additional graph construction and matrix inversion steps in LPA introduce only negligible computational overhead.

5. **Experiments on larger datasets**: As suggested, we extended our evaluation to two larger GLUE benchmark datasets, SST-2 and QNLI. GLUE is a widely used benchmark for evaluating models such as BERT and RoBERTa, and these datasets were selected for their larger size, diverse natural language tasks, and well-established evaluation protocols. We have included the results for SST-2 using BERT-base-uncased and DistilBERT-base, and we will report the results for RoBERTa-large and QNLI across all three model variants in the camera-ready version.
## SST2 Results with BERT-base-uncased  

| Method   | Accuracy |
|----------|----------|
| CE       | 91.2     |
| CE+SCL   | 92.1     |
| CE+GLOSS-O |**92.32**|