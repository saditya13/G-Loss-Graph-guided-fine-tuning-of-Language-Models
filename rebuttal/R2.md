1. We thank the reviewer for their valuable feedback and for acknowledging the novelty, clear motivation, and thoroughness of our experimental setup.


2. **Limited performance gains**: While the reported improvements may appear modest, they are consistent across five benchmarks and three language models of different sizes.


3. **Statistical significance**: To address this, we repeated the integrated approach experiments using three different random seeds, and the updated results are presented in Table 2. As observed, G-Loss consistently matches or outperforms other methods across all datasets. Due to time and computational constraints during the rebuttal period, we were unable to perform a multi-seed evaluation for the standalone mode; however, we plan to include these results in the camera-ready version.


4. **Efficiency analysis**: The average time per epoch and total training time are now reported in Table 3, with a detailed timing breakdown of the overall training process included in Figure 5 of the appendix. These results show that G-Loss does not introduce noticeable computational overhead during graph construction and LPA operations, as the graph is limited to a mini-batch scale.


5. **Experiments on larger datasets**: As suggested, we extended our evaluation to two larger GLUE benchmark datasets, SST-2 and QNLI. GLUE is a widely used benchmark for evaluating models such as BERT and RoBERTa, and these datasets were selected for their larger size, diverse natural language tasks, and well-established evaluation protocols.
## SST2 Results with BERT-base-uncased  

| Method   | Accuracy |
|----------|----------|
| CE       | 91.2     |
| CE+SCL   | 92.1     |
| CE+GLOSS-O |**92.32**|