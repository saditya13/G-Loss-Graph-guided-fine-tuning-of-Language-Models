1. We thank the reviewer for recognizing the novelty and thorough experimentation setup.
2. Limited performance gains: Same as point 2 in reviewer 1.
3. No statistical significance tests:  We reran our results with 3 or 5  random seeds, and the updated results are as follows: 
4. Efficiency calculation: Average per epoch time and Total training time are reported as follows.



Questions: 
1.How statistically significant are the reported improvements? Could you provide variance over multiple runs with random seeds?
Experiment of multiple seeds

2. Can you provide actual computational overhead of G-Loss (time per epoch, ablations on LPA and graph construction), then do a fair compare to CE or SCL?  (The authors claim the method is efficient but only provide "fewer epochs" without reporting Wall-clock time, since LPA and graph reconstruction is also time consuming.)
Experiment of runtime of losses.

3. Can you provide additional experiment on larger datasets?
	Our focus in this submission was on standard text classification benchmarks, however as suggested, we will extend our evaluation to  xxx dataset (AG news 1 million documents/3 classes). 