1. We thank the reviewer for their valuable feedback and for acknowledging the novelty, clear motivation, and thoroughness of our experimental setup.


2. **Limited performance gains**: While the reported improvements may appear modest, they are consistent across five benchmarks and three language models of different sizes.+

While the quantitative gains are modest, G-Loss contributes a conceptually clean and general mechanism for incorporating global structure into fine-tuning — something existing loss formulations do not achieve. Its consistent improvements across models, faster convergence, and better embedding coherence suggest that the benefit lies in representation quality rather than raw accuracy.


3. **Statistical significance tests**: To address this, we repeated the integrated approach experiments using three different random seeds, and the updated results are presented in Table 2. As observed, G-Loss consistently matches or outperforms other methods across all datasets. Due to time and computational constraints during the rebuttal period, we were unable to perform a multi-seed evaluation for the standalone mode; however, we plan to include these results in the camera-ready version.

We will also run the statistical test and report the results.

4. **Efficiency analysis**: As suggested we have added the average time per epoch and total training time in Table 3, add content/number or interpretation of gloss timings from that plot.    These results show that G-Loss does not introduce noticeable computational overhead during graph construction and LPA operations, as the graph is limited to a mini-batch scale. 



with a detailed timing breakdown of the overall training process included in Figure 5 of the appendix. These results show that G-Loss does not introduce noticeable computational overhead during graph construction and LPA operations, as the graph is limited to a mini-batch scale.  




5. **Experiments on larger datasets**: As suggested, we extended our evaluation to two larger GLUE benchmark datasets, SST-2 and QNLI. GLUE is a widely used benchmark for evaluating models such as BERT and RoBERTa, and these datasets were selected for their larger size, diverse natural language tasks, and well-established evaluation protocols.  We are running on QNLI and will report the results in camera-ready.
## SST2 Results with BERT-base-uncased  

| Method   | Accuracy |
|----------|----------|
| CE       | 91.2     |
| CE+SCL   | 92.1     |
| CE+GLOSS-O |**92.32**|