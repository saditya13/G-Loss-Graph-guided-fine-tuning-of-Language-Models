Reviewer VcWK06

We thank the reviewer for the constructive and detailed feedback and for recognizing the importance of incorporating global structural relationships into fine-tuning. We appreciate the reviewer’s careful comments on theoretical grounding, computational analysis, and experimental design, which have helped us improve the paper.

1. Conceptual novelty
   
While G-Loss builds upon well-established components such as Gaussian similarity and label propagation, its novelty lies in embedding the propagation mechanism directly into the fine-tuning objective of transformer models. This integration enables the graph structure to evolve dynamically with embeddings during training, yielding a differentiable, inductive, and memory-efficient loss formulation. To our knowledge, G-Loss is the first approach that incorporates label propagation in a closed-form, differentiable manner within the loss function rather than as a post-processing step. This enables the fine-tuning process to enforce global structure alignment rather than locally.

2. Computational complexity analysis
   
We have added the average time per epoch and the total training time to Table 3. Also, figure 5 in the appendix now provides a breakdown of the per-epoch training time of G-Loss and other baselines. The results show that G-Loss adds negligible computational overhead during graph construction (0.18 seconds per epoch) and LPA operations (0.07 seconds per epoch), since the graph is confined to the mini-batch scale. The forward pass per-epoch time for G-Loss (9.20 seconds) remains comparable to or lower than that of other losses, such as SCL (10.41 seconds) and Triplet Loss (9.15 seconds). Because G-Loss operates at the mini-batch level, the memory requirements for adjacency matrix storage and transition matrix computation are low. 

3. Dataset scale and generalizability

Due to computational resource constraints, the initial experiments were conducted on five small but widely used benchmark datasets. To further validate the scalability and generalizability of G-Loss, we added experiments on SST-2 (67K training samples) using BERT-base-uncased and DistilBERT-base. The results, shown below, demonstrate consistent improvements. We are currently extending experiments to RoBERTa-large and QNLI (105k training samples) across all three model variants, which will be included in the camera-ready version.

| Method            | BERT-base | DistilBERT |
| :---------------- | :-------- | :--------- |
| CE                | 91.2      | 90.14      |
| CE + SCL          | 92.1      | 90.36      |
| **CE + G-Loss-O** | **92.32** | **91.06**  |

These results confirm that G-Loss maintains its benefits on larger and more diverse datasets, indicating that the approach generalizes beyond small-scale benchmarks.

4. Comparison to BertGCN and other GNN architectures

We compared our approach with BertGCN, a strong graph-based baseline, using results from the official implementation, which is publicly available for full reproducibility. Since G-Loss introduces a new loss function rather than a new architecture, comparing it with standard objectives such as CE, SCL, Triplet, and Cosine Similarity is the most direct methodological evaluation. Nonetheless, we plan to extend our comparisons to additional graph-based architectures and recent contrastive learning methods, such as SimCSE and DeCLUTR, in future work.

5. Hyperparameter tuning and sensitivity

We appreciate the reviewer’s comments regarding the Optuna-based tuning for $\sigma$ in G-Loss-O. We show the sensitivity analysis for $\gamma$ (Figure 2a) across a wider range (0.1–0.9), confirming stable performance for $\gamma$ within [0.5, 0.7]. Section G of the appendix now includes a detailed hyperparameter guide that outlines parameter roles, recommended ranges, and observed stability across datasets to enhance reproducibility.

We thank the reviewer again for the constructive feedback. The added computational complexity analysis, expanded evaluations, and refined ablation studies strengthen both the theoretical and empirical aspects of G-Loss. These updates confirm that G-Loss is a theoretically grounded and efficient fine-tuning objective that effectively embeds global structural alignment into transformer training while maintaining low computational overhead and strong scalability.
