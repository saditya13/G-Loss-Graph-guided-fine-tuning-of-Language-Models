Reviewer VcWK06

We thank the reviewer for the constructive and detailed feedback and for recognizing the importance of incorporating global structural relationships into fine-tuning. We appreciate the reviewer’s careful comments on theoretical grounding, computational analysis, and experimental design, which have helped us improve the paper.

1. Conceptual novelty
   
While G-Loss builds upon well-established components such as Gaussian similarity and label propagation, its novelty lies in embedding the propagation mechanism directly into the fine-tuning objective of transformer models. This integration enables the graph structure to evolve dynamically with embeddings during training, yielding a differentiable, inductive, and memory-efficient loss formulation. To our knowledge, G-Loss is the first approach that incorporates label propagation in a closed-form, differentiable manner within the loss function rather than as a post-processing step. This enables the fine-tuning process to enforce global structure alignment continuously rather than locally.

2. Computational complexity analysis
   
We have added the average time per epoch and the total training time to Table 3. Figure 5 in the appendix now provides a detailed breakdown of the per-epoch training time. The results show that G-Loss adds negligible computational overhead during graph construction (0.18 seconds per epoch) and LPA operations (0.07 seconds per epoch), since the graph is confined to the mini-batch scale. The total per-epoch training time for G-Loss (9.20 seconds) remains comparable to or lower than that of other losses, such as SCL (10.41 seconds) and Triplet Loss (9.15 seconds). Because G-Loss operates at the mini-batch level, the memory requirements for adjacency matrix storage and transition matrix computation are low (approximately 1.04 $\times$ CE). 

3. Dataset scale and generalizability

Due to computational resource constraints, the initial experiments were conducted on five small but widely used benchmark datasets. To further validate the scalability and generalizability of G-Loss, we added experiments on SST-2 using BERT-base-uncased and DistilBERT-base. The results, shown below, demonstrate consistent improvements. We are currently extending experiments to RoBERTa-large and QNLI across all three model variants, which will be included in the camera-ready version.

| Method            | BERT-base | DistilBERT |
| :---------------- | :-------- | :--------- |
| CE                | 91.2      | 90.14      |
| CE + SCL          | 92.1      | 90.36      |
| **CE + G-Loss-O** | **92.32** | **91.06**  |

These results confirm that G-Loss maintains its benefits on larger and more diverse datasets, indicating that the approach generalizes beyond small-scale benchmarks.

Comparison to BertGCN and other GNN architectures

We compared our approach with BertGCN, a strong graph-based baseline, using results from the official implementation, which is publicly available for full reproducibility. Since G-Loss introduces a new loss function rather than a new architecture, comparing it with standard objectives such as CE, SCL, Triplet, and Cosine Similarity is the most direct methodological evaluation. Nonetheless, we plan to extend comparisons to additional graph-based architectures and recent contrastive learning methods such as SimCSE and DeCLUTR in future work.

Hyperparameter tuning and sensitivity

We appreciate the reviewer’s comments regarding the Optuna-based tuning for σ in G-Loss-O. To ensure fairness, we have re-run all baselines using identical Optuna search configurations. We also expanded the sensitivity analysis for γ (Figure 7) across a wider range (0.2–0.8), confirming stable performance for γ within [0.5, 0.7]. Section G of the appendix now includes a detailed hyperparameter guide outlining parameter roles, recommended ranges, and observed stability across datasets to improve reproducibility.

We thank the reviewer again for the constructive feedback. The added computational complexity analysis, expanded evaluations, and refined ablation studies strengthen both the theoretical and empirical aspects of G-Loss. These updates confirm that G-Loss is a theoretically grounded and efficient fine-tuning objective that effectively embeds global structural alignment into transformer training while maintaining low computational overhead and strong scalability.
