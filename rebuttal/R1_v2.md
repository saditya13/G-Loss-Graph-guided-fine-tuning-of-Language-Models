We thank the reviewer for their constructive feedback and appreciate the recognition of our paperâ€™s clear motivation for shifting from local to global structural optimization, comprehensive experimental design, and deeper analysis of the learned embedding space beyond standard performance metrics.

1. Marginal Performance Gains
While the performance gains are modest, they are consistent across five benchmark datasets and three language models of varying sizes. The goal of G-Loss is to introduce a loss function that captures global semantic relationships beyond what traditional loss functions can achieve. Our analyses show that G-Loss improves representation quality (as reflected in higher Silhouette scores), achieves faster convergence (fewer epochs), and reduces per-epoch training time through mini-batch-level graph construction, thereby balancing effectiveness and efficiency.

2. Statistical significance 
We ran additional experiments for the integrated mode using multiple random seeds. The updated results (Table 2) report mean and variance across runs, confirming that G-Loss consistently performs on par with or better than other loss functions across all datasets. While the variance remains low, we plan to include formal statistical significance tests (e.g., paired t-test or Wilcoxon signed-rank test) in the camera-ready version to confirm that the observed improvements are not due to random variation. Due to the limited rebuttal timeframe and computational constraints, we could not yet extend this multi-seed analysis to the standalone mode, but we will include it along with the significance tests in the final submission.

3. 
