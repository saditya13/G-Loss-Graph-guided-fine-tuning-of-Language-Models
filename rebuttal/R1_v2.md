Reviewer pE6A

We thank the reviewer for their constructive feedback and appreciate the recognition of our paper's clear motivation for shifting from local to global structural optimization, comprehensive experimental design, and deeper analysis of the learned embedding space beyond standard performance metrics.

1. Marginal performance gains
   
While the performance gains are modest, they are consistent across five benchmark datasets and three language models of varying sizes. The goal of G-Loss is to introduce a loss function that captures global semantic relationships which is not explicitly modeled by conventional objectives such as SCL (the closest competitor), Triplet, or Cosine Similarity. Our analyses show that G-Loss improves representation quality (*as reflected in higher Silhouette scores*), achieves faster convergence (*fewer epochs*), and reduces per-epoch training time through *mini-batch-level graph construction*. Together, these aspects yield a better trade-off between effectiveness and efficiency.
The table below summarizes the quantitative comparison of G-Loss-O and SCL across five benchmark datasets in terms of both representation quality and training efficiency:

| Dataset     | **Silhouette Score (↑)** |         | **Average Time per Epoch (s, ↓)** |         | **Observations**                |
| ----------- | ------------------------ | ------- | --------------------------------- | ------- | ------------------------------- |
|             | **G-Loss-O**             | **SCL** | **G-Loss-O**                      | **SCL** |                                 |
| **MR**      | 0.5507                   | 0.5725  | 27.77                             | 33.04   | −3.8% Sil ↓ / **16.0% faster**  |
| **R8**      | 0.7870                   | 0.4869  | 21.77                             | 25.56   | **+61.6% Sil ↑ / 14.8% faster** |
| **R52**     | 0.4824                   | 0.4561  | 25.87                             | 30.41   | **+5.8% Sil ↑ / 14.9% faster**  |
| **Ohsumed** | 0.1468                   | 0.1373  | 13.50                             | 15.70   | **+6.9% Sil ↑ / 14.0% faster**  |
| **20NG**    | 0.5849                   | 0.5507  | 46.10                             | 52.76   | **+6.2% Sil ↑ / 12.6% faster**  |


2. Statistical significance
   
We ran additional experiments for the integrated mode using multiple random seeds. The updated results (Table 2) report mean and variance across runs, confirming that *G-Loss consistently performs on par with or better than other loss functions across all datasets*. While the variance remains low, we plan to include formal statistical significance tests (e.g., paired t-test or Wilcoxon signed-rank test) in the camera-ready version to confirm that the observed improvements are not due to random variation. Due to the limited rebuttal timeframe and computational constraints, we could not yet extend this multi-seed analysis to the standalone mode, but we will include it along with the significance tests in the final submission.

3. Computational overhead
   
We have added the average time per epoch and total training time in Table 3, which show that G-Loss achieves faster convergence with a comparable per-epoch training cost. A detailed timing breakdown is presented in Figure 5, showing that one training epoch of G-Loss requires 9.20 seconds in total, consisting of 8.95 seconds for the BERT forward pass, 0.18 seconds for graph construction, and 0.07 seconds for LPA operations. For comparison, the per-epoch times for other loss functions are: Supervised Contrastive Loss (10.41 s), Triplet Loss (9.15 s), and Cosine Similarity Loss (9.09 s). These results demonstrate that G-Loss introduces negligible computational overhead, as *the additional graph and LPA computations account for less than 3% of total training time* while still achieving faster overall convergence.

4. Hyperparameter complexity and sensitivity
   
We appreciate the reviewer's concern regarding the added hyperparameters ($\lambda$, $\gamma$, $\sigma$). These parameters follow standard roles in graph-based learning- $\lambda$ balances supervised and structural loss, $\gamma$ controls label hiding in LPA, and $\sigma$ defines kernel width. To clarify their behavior, we have added a detailed guide in Section G of the appendix and summarized recommended ranges in Table 6 of the appendix. Our ablation results show that *G-Loss remains stable within narrow and consistent ranges across datasets* ($\gamma$: 0.5-0.7, $\lambda$: 0.7-0.9), indicating that the effective tuning space is small. This stability substantially reduces the need for complex tuning. Additionally, we will include a tornado plot in the camera-ready version to visualize parameter sensitivity and confirm that model performance and convergence vary minimally within these intervals.

We thank the reviewer for the constructive feedback. The additional analyses reaffirm that G-Loss is efficient, stable, and effective in modeling global semantic structure.
