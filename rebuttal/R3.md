1. Thankyou for acknowledging the multiple strengths  such as global semantic capture, memory efficient (dynamic mini-batch), inductive, wide and thorough experiments on 5 benchmarks across three different language models.

2. **Risk of dynamic graph instability**: The sensitivity arises because σ governs similarity scaling, which is expected in kernel-based methods.

3.**Lack of convergence proof**: We thank the reviewer for the insightful comment regarding the convergence and stability of the LPA formulation within G-Loss. The closed-form solution we employ follows the standard formulation in graph-based semi-supervised learning (Zhu et al., 2003; Zhou et al., 2004). When similarities are computed using the Gaussian kernel, the resulting similarity matrix $W$ is positive semidefinite, ensuring that the derived normalized transition matrix $T = D^{-1}W$ yields a row-stochastic or substochastic block $T_{uu}$ for unlabeled nodes. Because $T_{uu}$ is substochastic, its spectral radius $\rho(T_{uu})<1$ (as established in Eq. 7 of the original LPA derivation), implying that $(I-T_{uu})$ is non-singular. Consequently, the closed-form expression $F_u = (I-T_{uu}^{-1}T_{ul}Y_l) is well-defined and numerically stable under the practical assumptions of our mini-batch construction. Moreover, as established in LPA theory, all entries of $T$ are strictly positive due to the Gaussian kernel, ensuring that the constructed graph contains no isolated nodes, thereby preserving the validity of the propagation process.

4. Lack of domain generalization: (Paper fail to cover vision and multimodal tasks) - Our focus in this submission was on standard text classification benchmarks, however as suggested, we will extend our evaluation to  xxx dataset. Results on image dataset. 

5. Hyperparameter Sensitivity and Tuning Cost In terms of γ (label hiding ratio)
