1. Thankyou for acknowledging the multiple strengths  such as global semantic capture, memory efficient (dynamic mini-batch), inductive, wide and thorough experiments on 5 benchmarks across three different language models.
2. Risk of dynamic graph instability: The sensitivity arises because σ governs similarity scaling, which is expected in kernel-based methods. 
3. Lack of convergence proof: our constructed graph in a mini-batch in fully connected. 
4. Lack of domain generalization: (Paper fail to cover vision and multimodal tasks) - Our focus in this submission was on standard text classification benchmarks, however as suggested, we will extend our evaluation to  xxx dataset. Results on image dataset. 
5. Hyperparameter Sensitivity and Tuning Cost In terms of γ (label hiding ratio)
