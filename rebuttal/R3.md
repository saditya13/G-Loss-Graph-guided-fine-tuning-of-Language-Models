1.Thankyou for acknowledging the global semantic capture, memory efficient (dynamic mini-batch), inductive, wide and thorough experiments on 5 benchmarks across three different language models.

2.**Risk of dynamic graph instability**:
In G-Loss, the use of a Gaussian kernel inherently constrains edge weights to the bounded range (0,1], ensuring that incremental changes in embeddings produce smooth and continuous updates to the graph structure. Empirically, we observe low variance across random seeds (â‰¤1%), indicating that the dynamic graph construction does not introduce instability in practice. However, we find the reviewer's suggestion insightful and adding an additionial parameter controlliing the graph smoothing is an intersting idea and we plan to explore the same in the future. 

We appreciate the reviewer's insightful observation regarding the potential instability introduced by dynamically evolving graphs. 
of incorporating graph smoothing or moving-average mechanisms particularly valuable. We plan to explore this direction in future work to enhance the robustness of G-Loss under highly dynamic embedding updates.

3.**Lack of convergence proof**: We appreciate the observation. The closed-form solution we employ follows the standard formulation in graph-based semi-supervised learning (Zhu et al., 2003; Zhou et al., 2004). In GLoss, when similarities are computed using the Gaussian kernel, the resulting similarity matrix $W$ is positive semi-definite, ensuring that the derived normalized transition matrix $T = D^{-1}W$ (section 2.3 LPA) yields a row-stochastic/substochastic block $T_{uu}$ for unlabeled nodes. Because $T_{uu}$ is substochastic, its spectral radius $\rho(T_{uu})<1$ (as established in Eq. 7 of the original LPA derivation [Zhu et.al.2002]), implying that $(I-T_{uu})$ is non-singular. Consequently, the closed-form expression $F_u = (I-T_{uu}^{-1}T_{ul}Y_l)$ is well-defined and numerically stable under the practical assumptions of our mini-batch construction. Moreover, as established in LPA[Zhu et.al. 2002], all entries of $T$ are strictly positive due to the Gaussian kernel, ensuring that the constructed graph contains no isolated nodes, thereby preserving the validity of the propagation process. We have added a formal explanation in Section E of the Appendix, outlining the standard linear algebraic results involving the Neumann series expansion and the spectral radius condition for substochastic matrices. 

4.**Lack of domain generalization**: While we acknowledge that our current experiments are limited to text classification and NLP-related tasks. Conceptually, G-Loss can be applied to settings where encoder-generated embeddings are available for graph construction. We agree that our present claims regarding the applicability of G-Loss to other domains (e.g., computer vision or multimodal tasks) were not empirically validated. Accordingly, we will revise the relevant text to ensure that our claims are in line with our experiments.

5.**Hyperparameter tuning guide**: A systematic list of hyperparameters , their optimal ranges and tuning guide has been added in section G of appendix. 
