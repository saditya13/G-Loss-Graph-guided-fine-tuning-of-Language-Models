We thank the reviewer for their helpful feedback and appreciate the acknowledgement of our work's clear motivation, comprehensive evaluation on integrated and standalone modes, and thorough analysis and ablation of the proposed G-Loss function.
**Marginal performance gains**: Although the reported improvements appear marginal, they are consistent across the five benchmarks and three language models of varying sizes. Beyond accuracy/F1, embedding quality improves significantly, as shown by the silhouette score [Table 3] and t-SNE visualization[Figure in appendix], confirming the benefit of enforcing global semantic information. 
**Average over multiple random seeds**: We have rerun the experiments using three random seeds for the integrated approach, and the updated results are presented in Table 2. As observed, G-Loss performs on par with or outperforms other approaches across all datasets. Due to the limited rebuttal timeframe and computational constraints, we could not extend this multi-seed analysis to the standalone mode; however, we plan to include these results in the camera-ready version.
**Unquantified Computational Overhead**: A detailed timing breakdown of the overall training process has been included in Figure 5 of the appendix. As shown, G-Loss introduces no noticeable computational overhead during graph construction, normalization, or matrix inversion, since the graph size is limited to the mini-batch scale. Furthermore, the average time per epoch and total training time are now reported in Table 3 for a comprehensive comparison.

4.**High Complexity in Hyperparameter Tuning**: Parameters (λ, γ, σ) are unintuitive and sensitive. 
While our method introduces λ, γ, and σ, these parameters are natural and commonly used in graph learning (λ - balance weight between supervised and structural loss,  γ - label hiding ratio similar to ratio of unlabeled nodes in LPA, σ- kernel width). These parameters are natural and common to graph learning. It is true that our ablation shows sensitivity on Ohsumed with respect to γ. However, this behavior is expected and dataset-dependent, since Ohsumed has highly skewed class distribution. As shown in the heatmaps in Figure 2 (a and b), performance is stable across reasonable ranges of γ (in 0.5-0.7), λ (), and σ (stable within ±0.5x of optimal, with drops occurring only at extreme values). To improve clarity for practitioners, we have now included practical guidelines in the manuscript [Line 293].
