1.We thank the reviewer for their helpful feedback and appreciate the acknowledgement of our work's clear motivation, comprehensive evaluation on integrated and standalone modes, and thorough analysis and ablation of the proposed G-Loss function.

2.**Marginal performance gains**: While the reported gains in accuracy and F1-score may appear modest, they are consistent across five benchmark datasets and three language models of varying sizes. The motivation is to introduce a loss function that extends beyond local pairwise optimization (as in Triplet or SCL losses) to incorporate global graph-structured relationships. This global formulation enables faster convergence (less epochs to convergence), enhances representation quality (better Silhouette score), incurs negligible computational overhead (less per epoch train time) due to the mini-batch scale of graph construction.

3.**Average over multiple random seeds**: As suggested, we reran the experiments for the integrated approach and report the mean and variance across three different random seeds in Table 2. The results remain consistent with our earlier findings, with G-Loss performing on par or outperforming other loss functions across all datasets and language models. Owing to the limited rebuttal timeframe and computational constraints, we will include the results for the standalone approach with multiple seeds in the camera-ready version. 


We have rerun the experiments using three random seeds for the integrated approach, and the updated results are presented in Table 2. As observed, G-Loss performs on par with or outperforms other approaches across all datasets. Due to the limited rebuttal timeframe and computational constraints, we could not extend this multi-seed analysis to the standalone mode; however, we will these results in the camera-ready version.  (change the lang)

4.**Unquantified Computational Overhead**: A detailed timing breakdown of the overall training process has been included in Figure 5 of the appendix. As shown, G-Loss introduces no noticeable computational overhead during graph construction, normalization, or matrix inversion, since the graph size is limited to the mini-batch scale. Furthermore, the average time per epoch and total training time are now reported in Table 3 for a comprehensive comparison.

5.**High Complexity in Hyperparameter Tuning**: Parameters (λ, γ, σ) are unintuitive and sensitive. 
While our method introduces λ, γ, and σ, these parameters are commonly used in graph learning (λ - balances weight between supervised (CE) and structural loss,  γ - label hiding ratio similar to ratio of unlabeled nodes in LPA, σ- kernel width). 

It is true that our ablation shows sensitivity on Ohsumed with respect to γ. However, this behavior is expected and dataset-dependent, since Ohsumed has highly skewed class distribution. As shown in the heatmaps in Figure 2 (a and b) & Fig xxx (lambda heatmap), performance is stable across reasonable ranges of γ (in 0.5-0.7), λ (0.7-0.9), and σ (stable within ±0.5x of optimal, with drops occurring only at extreme values). Moreover, we have proposed two approaches of G-Loss - G-Loss-SQRT and G-Loss-O. G-Loss-SQRT approach is suitable for resource constrained cases  as sigma in this case can be coomputed mathematically beforehand, whereas in cases where performance is more desriable optuna-based tuning for G-Loss-O is suitable. To improve clarity for practitioners, we have included a systematic list of hyperparameters , their optimal ranges and tuning guide in section XXX of appendix. 




While G-loss brings additional parameters, we have provided guidance on how to tune these parameters to obtain stable performance. We have also added a table describing the systematic list of hyperparameters and optial ranges in table xxx. We will also provide a tornado plot showing the sensitivity of the parameters on the performance, and convergence (TIME taken, check it out, if it applies) ..for example tuning k in k nearest might take more time than tuning the distance measure.
