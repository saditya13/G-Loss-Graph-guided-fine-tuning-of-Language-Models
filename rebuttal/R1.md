1. We appreciate the acknowledgement of our work's clear motivation, comprehensive evaluation, and thorough analysis of the proposed G-Loss function.
2. **Marginal performance gains**: Although the reported improvements appear marginal, they are consistent across the five benchmarks and three language models of varying sizes. Beyond accuracy/F1, embedding quality improves significantly, as shown by the silhouette score [Table3] and t-SNE visualization[figure4 in appendix], confirming the benefit of enforcing global semantic information. 

We reran our results with 3 random seeds, and the updated results are as follows:



### DistilBERT-base Results Across Datasets

| Loss Function     | MR (Acc / F1)      | R8 (Acc / F1)       | R52 (Acc / F1)      | 20NG (Acc / F1)     | Ohsumed (Acc / F1)   |
|-------------------|--------------------|----------------------|----------------------|----------------------|----------------------|
| CE                | 84.85±0.23 / 84.85±0.23 | 97.48±0.11 / 93.02±0.48 | 95.78±0.04 / 83.64±0.69 | 83.15±0.61 / 82.69±0.56 | 69.50±0.34 / 61.56±0.29 |
| GLoss-SQRT + CE| **85.14±0.33 / 85.14±0.34** | 97.46±0.17 / 93.62±0.69 | **96.13±0.34** / 83.93±0.35 | 83.61±0.23 / 83.16±0.20 | 69.93±0.38 / 62.19±0.21 |
| GLoss-O + CE      | 85.10±0.45 / 85.10±0.45 | **97.71±0.12 / 93.61±0.22** | 96.07±0.04 / **84.44±0.43** | **83.64±0.28 / 83.23±0.27** | **70.16±0.21 / 62.41±0.34** |
| SCL + CE          | 84.16±0.46 / 84.16±0.46 | 97.65±0.25 / 93.79±0.43 | 96.17±0.10 / 83.82±1.62 | 83.49±0.20 / 83.18±0.12 | 70.02±0.35 / 60.56±0.30 |

### BERT-base-uncased Results Across Datasets


3. **Unquantified Computational Overhead**:  Average per epoch time and Total training time with BERT-base-uncased are reported as follows.
### BERT-base-uncased Results
### BERT-base-uncased Results

| Dataset | Loss Function | Accuracy | F1 Score | Test Sil Score | Total Train Time (s) | Avg Time / Epoch (s) | Early Stop Epoch |
|----------|----------------|-----------|-----------|----------------|----------------------|----------------------|------------------|
| **MR** | Triplet Loss | **86.66** | **86.66** | 0.4479 | 941.55 | **27.50** | 29 |
| | Cos-Sim | 86.41 | 86.41 | 0.5331 | **654.56** | 27.70 | **20** |
| | SCL | 86.56 | 86.56 | **0.5725** | 1258.41 | 33.04 | 35 |
| | GLoss-O | 86.61 | 86.61 | 0.5507 | 810.85 | 27.77 | 25 |
| **R8** | Triplet Loss | 97.67 | 93.96 | 0.6578 | 727.85 | **21.19** | 30 |
| | Cos-Sim | 97.94 | 93.64 | 0.7380 | 4960.64 | 21.65 | 200 |
| | SCL | 97.85 | 94.10 | 0.7869 | 1088.58 | 25.56 | 38 |
| | GLoss-O | **97.99** | **94.25** | **0.7870** | **572.29** | 21.77 | **23** |
| **R52** | Triplet Loss | **96.23** | **82.05** | **0.4658** | 5496.14 | **25.05** | 189 |
| | Cos-Sim | 96.18 | 80.90 | 0.3969 | 1925.82 | 25.79 | 64 |
| | SCL | 96.10 | 81.43 | 0.4536 | 1170.50 | 30.41 | **34** |
| | GLoss-O | **96.24** | **81.99** | 0.4022 | **1095.93** | 25.87 | 39 |
| **Ohsumed** | Triplet Loss | 68.56 | 57.86 | 0.1220 | 1576.44 | 13.09 | 105 |
| | Cos-Sim | 70.49 | 59.51 | 0.1393 | 3064.31 | 13.42 | 200 |
| | SCL | 69.85 | 57.12 | 0.1370 | 691.03 | 15.70 | **39** |
| | GLoss-O | **70.67** | **62.52** | **0.1468** | **633.50** | 13.50 | 41 |
| **20NG** | Triplet Loss | 84.16 | 83.78 | 0.4538 | **8516.99** | **43.46** | 171 |
| | Cos-Sim | 84.20 | 83.72 | 0.5524 | 10155.44 | 45.10 | 200 |
| | SCL | 84.19 | 83.75 | 0.5507 | 11671.00 | 52.76 | 200 |
| | GLoss-O | **84.99** | **84.53** | **0.5849** | 8596.50 | **46.10** | **180** |



4. **High Complexity in Hyperparameter Tuning**: Parameters (λ, γ, σ) are unintuitive and sensitive. 
While our method introduces λ, γ, and σ, these parameters are natural and commonly used in graph learning (λ - balance weight between supervised and structural loss,  γ - label hiding ratio similar to ratio of unlabeled nodes in LPA, σ- kernel width). These parameters are natural and common to graph learning. It is true that our ablation shows sensitivity on Ohsumed with respect to γ. However, this behavior is expected and dataset-dependent, since Ohsumed has highly skewed class distribution. As shown in the heatmaps in Figure 2 (a and b), performance is stable across reasonable ranges of γ (in 0.5-0.7), λ (), and σ (stable within ±0.5x of optimal, with drops occurring only at extreme values). To improve clarity for practitioners, we have now included practical guidelines in the manuscript [Line 293].
