### Reviewer tzd214

We thank the reviewer for the thoughtful and detailed feedback and for recognizing the simplicity, clarity, and accessibility of our approach. We appreciate the acknowledgement that G-Loss provides a straightforward yet principled mechanism for global structure alignment through label propagation, achieving consistent performance across datasets and models. We are also grateful for the reviewer’s appreciation of the method’s intuitive design, its empirical effectiveness, and its potential applicability beyond text classification tasks.



1. Global semantic consistency

*Can you clarify the meaning of "global semantic consistency" is it the same as "better class clustering"?*
By "global semantic consistency," we refer to the structural alignment that G-Loss enforces beyond local neighbor relations. Specifically, G-Loss considers both labeled and unlabeled samples jointly, allowing the model to learn representations that exhibit class-level coherence across the mini-batch. This alignment extends beyond immediate neighbors (local consistency) to multi-hop relationships that capture the overall topology of the embedding graph. Indeed, as noted by the reviewer, this global alignment manifests as improved class clustering and clearer inter-class separation at both batch and dataset levels.

*Why do other contrastive learning approaches not enforce global consistency in expectation?*
Contrastive methods such as Supervised Contrastive Learning (SCL) optimize pairwise or triplet relationships by pulling together positive pairs and pushing apart negatives. While this formulation enforces local alignment, it does not jointly constrain all samples to maintain class-level topology across the embedding space. G-Loss differs in that it propagates label information through the constructed graph, imposing a global constraint on embedding similarity and ensuring consistent alignment of all samples within a batch, rather than optimizing only pairwise interactions.

*Can you also provide plots for the other remaining datasets?*
*Instead of relying on visual observations, can we resort to using some clustering score to validate the claim?*
We now include t-SNE visualizations for all datasets and epochs in Figure 6 of the appendix. To quantify representation quality, we report the Silhouette coefficient for all methods in Table 3, where G-Loss improves average Silhouette scores by 4.2 percent over SCL and 6.8 percent over CE, confirming more compact intra-class clusters and greater inter-class separation.



2. Computational efficiency

As suggested, we have added a detailed comparison of average time per epoch and total training time in Table 3 and a per-component breakdown in Figure 5. One epoch of G-Loss takes 9.20 seconds (BERT forward 8.95s, graph construction 0.18s, LPA 0.07s), compared to 10.41s for SCL and 9.15s for Triplet Loss. This shows that G-Loss maintains comparable per-epoch cost while converging faster overall. Memory usage increases marginally (about 1.04 times CE) since the graph is constructed dynamically at the mini-batch level.



3. Downstream tasks and larger language models

We agree that extending G-Loss beyond text classification to vision-language and multimodal settings would strengthen the contribution. While this was not feasible within the rebuttal period due to computational constraints, we plan to evaluate G-Loss with vision-language models such as CLIP and with language models exceeding 1B parameters (for example, DeBERTaV3-1.1B) under quantized fine-tuning in future work. These extensions will be detailed in the camera-ready version.



4. Choice of Gaussian kernel

The Gaussian kernel was selected because it is well-established in graph-based semi-supervised learning (Zhu et al., 2003 [1]; Zhou et al., 2003 [2]) and produces a positive semi-definite similarity matrix W. This ensures stability and invertibility of the LPA formulation, while naturally modeling smooth decay of influence between distant samples. Empirically, Gaussian-based graphs yield stable propagation and balanced connectivity across all benchmarks.



5. Optimizer clarification

We thank the reviewer for noticing the discrepancy regarding the optimizer. All experiments were conducted using Adam, which is standard for fine-tuning transformer-based models (BERT, RoBERTa). The mention of SGD was a typographical oversight during manuscript preparation. We have verified all scripts and logs to confirm Adam was used and have corrected the manuscript accordingly.



6. Visualization across epochs

As suggested, we added plots showing the evolution of embeddings across epochs (1, 20, 40, 60, 80), averaged across batches, in Appendix Figure 6. These visualizations illustrate gradual cluster consolidation rather than single-step snapshots, supporting the claim of progressive global alignment.



Summary

We thank the reviewer again for the helpful and detailed comments. The additional clustering metrics, efficiency analyses, visualizations, and clarifications strengthen the empirical support for G-Loss and make its conceptual foundation clearer. Together, these additions confirm that G-Loss provides a simple, interpretable, and computationally efficient framework for enforcing global semantic consistency during fine-tuning.



References
[1] Zhu, X., Ghahramani, Z., and Lafferty, J. D. (2003). Semi-supervised learning using Gaussian fields and harmonic functions. ICML.
[2] Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and Schölkopf, B. (2003). Learning with local and global consistency. NIPS.
