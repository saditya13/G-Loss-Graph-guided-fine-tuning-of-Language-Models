Reviewer tzd214

We thank the reviewer for the thoughtful and detailed feedback and for recognizing the simplicity, clarity, and accessibility of our approach. We appreciate the acknowledgement that G-Loss provides a straightforward yet principled mechanism for global structure alignment through label propagation, achieving consistent performance across datasets and models. We are also grateful for the reviewer’s appreciation of the method’s intuitive design, its empirical effectiveness, and its potential applicability beyond text classification tasks.

1. Global semantic consistency

*Can you clarify the meaning of "global semantic consistency", is it the same as "better class clustering"?*
By "global semantic consistency," we refer to the structural alignment that G-Loss enforces beyond local neighbor relations. Specifically, G-Loss considers both labeled and unlabeled samples jointly, allowing the model to learn representations that exhibit class-level coherence across the mini-batch. This alignment extends beyond immediate neighbors (local consistency) to multi-hop relationships that capture the overall topology of the embedding graph. Indeed, as noted by the reviewer, this global alignment manifests as improved class clustering and clearer inter-class separation at both batch and dataset levels.

| **Dataset**  | **Silhouette Score (↑)**  |         | **Observations**                |
|--------------|---------------------------|---------|---------------------------------|
|              | **G-Loss-O**              | **SCL** |                                 |
| **MR**       | 0.5507                    | 0.5725  | −3.8% Sil ↓                     |
| **R8**       | 0.7870                    | 0.4869  | **+61.6% Sil ↑**                |
| **R52**      | 0.4824                    | 0.4561  | **+5.8% Sil ↑**                 |
| **Ohsumed**  | 0.1468                    | 0.1373  | **+6.9% Sil ↑**                 |
| **20NG**     | 0.5849                    | 0.5507  | **+6.2% Sil ↑**                 |


*Why do other contrastive learning approaches not enforce global consistency in expectation?*
Contrastive methods, such as Supervised Contrastive Learning (SCL), optimize pairwise or triplet relationships by pulling together positive pairs and pushing apart negative pairs. While this formulation enforces local alignment, it does not jointly constrain all samples to maintain class-level topology across the embedding space. G-Loss differs in that it propagates label information through the constructed graph, imposing a global constraint on embedding similarity and ensuring consistent alignment of all samples within a batch, rather than optimizing only pairwise interactions.

*Which experiments should I refer to, in order to draw this conclusion?*
We report the macro-silhouette score in Table 3, which quantifies the intra-class compactness and inter-class separability of the learned embedding space. G-LOSS attains the highest silhouette values across most benchmark datasets. This empirically demonstrates that G-LOSS promotes the formation of globally coherent and well-separated clusters. 

*Can you also provide plots for the other remaining datasets?*
*Instead of relying on visual observations, can we resort to using some clustering score to validate the claim?*
add plot response

To quantify representation quality, we report the Silhouette coefficient for all methods in Table 3, where G-Loss improves average Silhouette scores by 4.2 percent over SCL and 6.8 percent over CE, confirming more compact intra-class clusters and greater inter-class separation.


*Instead of relying on visual observations, can we resort to using some clustering score to validate the claim?*
To quantitatively evaluate the clustering quality of the learned representations, we report the Silhouette coefficient for G-Loss and other baseline losses across all datasets in Table 3. 

2. Computational efficiency

As suggested, we have added a detailed comparison of average time per epoch and total training time in Table 3 and a per-component breakdown in Figure 5. One epoch of G-Loss takes 9.20 seconds (BERT forward 8.95s, graph construction 0.18s, LPA 0.07s), compared to 10.41s for SCL and 9.15s for Triplet Loss. This shows that G-Loss maintains comparable per-epoch cost while converging faster overall. Memory usage increases marginally (about 1.04 times CE) since the graph is constructed dynamically at the mini-batch level.

3. Downstream tasks and larger language models

We agree that extending G-Loss beyond text classification to vision-language and multimodal settings would strengthen the contribution. While this was not feasible within the rebuttal period due to computational constraints, we plan to evaluate G-Loss with vision-language models such as CLIP and with language models exceeding 1B parameters (for example, DeBERTaV3-1.1B) under quantized fine-tuning in future work. These extensions will be detailed in the camera-ready version.

4. Choice of Gaussian kernel

The Gaussian kernel was selected because it is well-established in graph-based semi-supervised learning (Zhu et al., 2003 [1]; Zhou et al., 2003 [2]) and produces a positive semi-definite similarity matrix W. This ensures stability and invertibility of the LPA formulation, while naturally modeling smooth decay of influence between distant samples. Empirically, Gaussian-based graphs yield stable propagation and balanced connectivity across all benchmarks.

5. Optimizer clarification

We thank the reviewer for noticing the discrepancy regarding the optimizer. We apologize for the miswriting; the mention of SGD was a typographical oversight during manuscript preparation. All experiments were conducted using Adam, which is standard for fine-tuning transformer-based models (BERT, RoBERTa). We have verified all scripts and logs to confirm Adam was used and have corrected the manuscript accordingly. 

6. Visualization across epochs

As suggested, we added plots showing the evolution of embeddings across epochs (1, 20, 40, 60, 80), averaged across batches, in Appendix Figure 6. These visualizations illustrate gradual cluster consolidation rather than single-step snapshots, supporting the claim of progressive global alignment.

We thank the reviewer again for the helpful and detailed comments. The additional clustering metrics, efficiency analyses, visualizations, and clarifications strengthen the empirical support for G-Loss and make its conceptual foundation clearer. These additions reaffirm that G-Loss is both theoretically sound and empirically validated as a practical, efficient, and general fine-tuning framework for enforcing global semantic consistency.

References
[1] Zhu, X., Ghahramani, Z., and Lafferty, J. D. (2003). Semi-supervised learning using Gaussian fields and harmonic functions. ICML.

[2] Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and Schölkopf, B. (2003). Learning with local and global consistency. NIPS.




2. which exp - sil silhouette
in the plots it snot properly seen due to dimensionilty reduction however results in table 2 plots are not showing much distinciton 
we can see from table 2 there is an marginal incerease in performance. we would like to present the plots generated from standalone training for R8 dataset
along with their silhouette scores. Whrere the improved cluster quality is clearly visible. For other datasets, it would be difficult to visauliza 
the better separation between classes for Gloss and SCL it is noticable from Tble 3 that an imrpovement of almost 6% in silhouette in 
in above table.

3. We have started other exp of  gloss on larger dtasets to test th eorbustness, we will try to put up the results on larger models >1B eg. Deberta
under quantized setting in camera ready given the resource contraints at our institute.
