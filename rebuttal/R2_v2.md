Reviewer HnVv

We thank the reviewer for their constructive feedback and appreciate the recognition of the novelty of embedding label propagation into the loss function, the clear motivation, and the thorough experimental setup of our work.

1. Limited performance gains
   
While the reported improvements are modest, they are *consistent across five benchmark datasets and three language models of different sizes*. 
The goal of G-Loss is to introduce a fine-tuning objective that incorporates global semantic structure, which conventional loss functions often fail to capture. 
Our analyses demonstrate that G-Loss yields more coherent embeddings, achieves faster convergence, and enhances representation quality. 
This indicates that its main contribution lies in enhancing the learned feature space, saving time, while optimizing the performance.

2. Statistical significance
   
We ran additional experiments for the integrated mode using multiple random seeds. The updated results (Table 2) *report the mean and variance across runs*, confirming that G-Loss consistently performs on par with or better than other loss functions across all datasets. 
While the variance remains low, we plan to include formal statistical significance tests such as the paired t-test or Wilcoxon signed-rank test in the camera-ready version to confirm that the observed improvements are not due to random variation. 
Due to the limited rebuttal timeframe and computational constraints, we could not yet extend this multi-seed analysis to the standalone mode; however, we will include it together with the significance tests in the final submission.

3. Computational efficiency
   
We have added the average time per epoch and total training time in Table 3, showing that G-Loss achieves faster convergence while maintaining a comparable per-epoch training cost. A detailed timing breakdown is provided in Figure 5, where *one training epoch of G-Loss requires a total of 9.20 seconds, consisting of 8.95 seconds for the BERT forward pass, 0.18 seconds for graph construction, and 0.07 seconds for LPA operations*. For comparison, the per-epoch times for other loss functions are: Supervised Contrastive Loss (10.41 s), Triplet Loss (9.15 s), and Cosine Similarity Loss (9.09 s). These results affirm that the additional graph and LPA computations introduce negligible overhead (*less than 3% of total training time*) while G-Loss converges in fewer epochs, resulting in lower overall training time.

4. Experiments on larger datasets

As suggested, we extended our evaluation to larger datasets from the GLUE benchmark, specifically SST-2 and QNLI. These datasets were selected for their larger scale, linguistic diversity, and established use in evaluating transformer-based models. We report the SST-2 results for BERT-base-uncased and DistilBERT-base in Table 4, showing consistent gains with G-Loss:

| Method            | BERT-base | DistilBERT |
| :---------------- | :-------- | :--------- |
| CE                | 91.2      | 90.14      |
| CE + SCL          | 92.1      | 90.36      |
| **CE + G-Loss-O** | **92.32** | **91.06**  |

These results indicate that G-Loss continues to improve performance even on larger and more complex datasets. We are currently running experiments on QNLI with all three model variants (**BERT-base-uncased**, **DistilBERT**, and RoBERTa-large) and will include these results in the camera-ready version.

We thank the reviewer again for the valuable feedback. The additional experiments and analyses reaffirm that G-Loss is an effective and efficient fine-tuning objective that scales well to larger datasets while maintaining stable and consistent performance.
