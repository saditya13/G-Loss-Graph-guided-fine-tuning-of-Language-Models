Reviewer HnVv

We thank the reviewer for their constructive feedback and appreciate the recognition of the novelty of embedding label propagation into the loss function, the clear motivation, and the thorough experimental setup of our work.

Limited performance gains

While the reported improvements are modest, they are consistent across five benchmark datasets and three language models of different sizes. 
The goal of G-Loss is to introduce a fine-tuning objective that incorporates global semantic structure, which conventional loss functions do not capture. 
Our analyses show that G-Loss produces more coherent embeddings, achieves faster convergence, and improves representation quality. 
This indicates that its main contribution lies in enhancing the learned feature space, saving time, while optimizing the performance.

We ran additional experiments for the integrated mode using multiple random seeds. The updated results (Table 2) report the mean and variance across runs, confirming that G-Loss consistently performs on par with or better than other loss functions across all datasets. 
While the variance remains low, we plan to include formal statistical significance tests such as the paired t-test or Wilcoxon signed-rank test in the camera-ready version to confirm that the observed improvements are not due to random variation. 
Due to the limited rebuttal timeframe and computational constraints, we could not yet extend this multi-seed analysis to the standalone mode, but we will include it together with the significance tests in the final submission.

