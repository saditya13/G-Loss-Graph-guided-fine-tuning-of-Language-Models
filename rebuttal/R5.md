**Computational complexity analysis**: We have added the average time per epoch and the total training time in Table 3, while also add Figure 5 in the appendix which provides a breakdown of the per epoch training time. These results indicate that G-Loss adds negligible computational overhead during graph construction (0.18 sec per epoch) and LPA operations (0.07 sec per epoch), since the graph is confined to the scale of a mini-batch. Moreover, G-Loss works at minibatch level, so the memory requiremnts for adjacency matrix storage and transition matrix computations are also not high.

**Small benchmark datasets:** Due to constraints in computational resources, we initially conducted experiments on five small but widely used benchmark datasets. To further validate the generalizability of our approach, we have included the results for SST-2 using BERT-base-uncased and DistilBERT-base, and will report the results for RoBERTa-large and QNLI across all three model variants in the camera-ready version.
## SST2 Results with BERT-base-uncased  

| Method   | Accuracy |
|----------|----------|
| CE       | 91.2     |
| CE+SCL   | 92.1     |
| CE+GLOSS-O |**92.32**|

**Comparison to BertGCN and other GNN architectures**: We compared our approach with BertGCN, the state-of-the-art on chosen benchmark datasets, using numbers reported in the original paper. The official code of BertGCN is publicly available for full reproducibility. Since our work primarily proposes a new loss function, comparing it against other well-established loss functions is methodologically relevant evaluation, rather than against newer GNN architectures or advanced frameworks, though we plan to explore such comparisons in future work.