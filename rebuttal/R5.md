**Computational complexity analysis**: Table 3 now reports the average time per epoch and the total training time, while Figure 5 in the appendix provides a detailed breakdown of the overall training process. These results indicate that G-Loss adds negligible computational overhead during graph construction and LPA operations, since the graph is confined to the scale of a mini-batch. Moreover, G-Loss works at minibatch level, so the memory requiremnts for adjacency matrix storage and transition matrix computations are not high.

**Small benchmark datasets:** Due to constraints in computational resources, we initially conducted experiments on a set of small but widely used benchmark datasets. To further validate the generalizability of our approach, we are including experiments on larger datasets, such as SST-2 (67K examples), and the corresponding results are as follows:
## SST2 Results with BERT-base-uncased  

| Method   | Accuracy |
|----------|----------|
| CE       | 91.2     |
| CE+SCL   | 92.1     |
| CE+GLOSS-O |**92.32**|

**Comparison to BertGCN and other GNN architectures**: We compared our approach with BertGCN, the state-of-the-art on chosen benchmark datasets, using numbers reported in the original paper. The official code of BertGCN is publicly available for full reproducibility. Since our work primarily proposes a new loss function, comparing it against other well-established loss functions is the most methodologically relevant evaluation, rather than against newer GNN architectures or advanced frameworks, though we plan to explore such comparisons in future work.